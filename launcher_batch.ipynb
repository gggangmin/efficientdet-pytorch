{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 작동 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.parallel\n",
    "from contextlib import suppress\n",
    "\n",
    "from effdet import create_model, create_evaluator, create_dataset, create_loader\n",
    "from effdet.data import resolve_input_config\n",
    "from timm.utils import AverageMeter, setup_default_logging\n",
    "from timm.models.layers import set_layer_config\n",
    "\n",
    "from matplotlib import pyplot as plt # for visualization\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import warnings\n",
    "from PIL import Image\n",
    "from torchvision.ops.boxes import batched_nms\n",
    "\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "has_apex = False\n",
    "try:\n",
    "    from apex import amp\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "has_native_amp = False\n",
    "try:\n",
    "    if getattr(torch.cuda.amp, 'autocast') is not None:\n",
    "        has_native_amp = True\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "def add_bool_arg(parser, name, default=False, help=''):  # FIXME move to utils\n",
    "    dest_name = name.replace('-', '_')\n",
    "    group = parser.add_mutually_exclusive_group(required=False)\n",
    "    group.add_argument('--' + name, dest=dest_name, action='store_true', help=help)\n",
    "    group.add_argument('--no-' + name, dest=dest_name, action='store_false', help=help)\n",
    "    parser.set_defaults(**{dest_name: default})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--results'], dest='results', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='JSON filename for evaluation results', metavar='FILENAME')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch ImageNet Validation')\n",
    "parser.add_argument('root', metavar='DIR',\n",
    "                    help='path to dataset root') # 디폴트 데이터셋 파싱\n",
    "parser.add_argument('--dataset', default='coco', type=str, metavar='DATASET',\n",
    "                    help='Name of dataset (default: \"coco\"') \n",
    "parser.add_argument('--split', default='val',\n",
    "                    help='validation split')\n",
    "parser.add_argument('--model', '-m', metavar='MODEL', default='tf_efficientdet_d1',\n",
    "                    help='model architecture (default: tf_efficientdet_d1)')\n",
    "add_bool_arg(parser, 'redundant-bias', default=None,\n",
    "                    help='override model config for redundant bias layers')\n",
    "add_bool_arg(parser, 'soft-nms', default=None, help='override model config for soft-nms')\n",
    "parser.add_argument('--num-classes', type=int, default=None, metavar='N',\n",
    "                    help='Override num_classes in model config if set. For fine-tuning from pretrained.')\n",
    "parser.add_argument('-j', '--workers', default=4, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 4)')\n",
    "parser.add_argument('-b', '--batch-size', default=128, type=int,\n",
    "                    metavar='N', help='mini-batch size (default: 128)')\n",
    "parser.add_argument('--img-size', default=None, type=int,\n",
    "                    metavar='N', help='Input image dimension, uses model default if empty')\n",
    "parser.add_argument('--mean', type=float, nargs='+', default=None, metavar='MEAN',\n",
    "                    help='Override mean pixel value of dataset')\n",
    "parser.add_argument('--std', type=float,  nargs='+', default=None, metavar='STD',\n",
    "                    help='Override std deviation of of dataset')\n",
    "parser.add_argument('--interpolation', default='bilinear', type=str, metavar='NAME',\n",
    "                    help='Image resize interpolation type (overrides model)')\n",
    "parser.add_argument('--fill-color', default=None, type=str, metavar='NAME',\n",
    "                    help='Image augmentation fill (background) color (\"mean\" or int)')\n",
    "parser.add_argument('--log-freq', default=10, type=int,\n",
    "                    metavar='N', help='batch logging frequency (default: 10)')\n",
    "parser.add_argument('--checkpoint', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--pretrained', dest='pretrained', action='store_true',\n",
    "                    help='use pre-trained model')\n",
    "parser.add_argument('--num-gpu', type=int, default=1,\n",
    "                    help='Number of GPUS to use')\n",
    "parser.add_argument('--no-prefetcher', action='store_true', default=False,\n",
    "                    help='disable fast prefetcher')\n",
    "parser.add_argument('--pin-mem', action='store_true', default=False,\n",
    "                    help='Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.')\n",
    "parser.add_argument('--use-ema', dest='use_ema', action='store_true',\n",
    "                    help='use ema version of weights if present')\n",
    "parser.add_argument('--amp', action='store_true', default=False,\n",
    "                    help='Use AMP mixed precision. Defaults to Apex, fallback to native Torch AMP.')\n",
    "parser.add_argument('--apex-amp', action='store_true', default=False,\n",
    "                    help='Use NVIDIA Apex AMP mixed precision')\n",
    "parser.add_argument('--native-amp', action='store_true', default=False,\n",
    "                    help='Use Native Torch AMP mixed precision')\n",
    "parser.add_argument('--torchscript', dest='torchscript', action='store_true',\n",
    "                    help='convert model torchscript for inference')\n",
    "parser.add_argument('--results', default='', type=str, metavar='FILENAME',\n",
    "                    help='JSON filename for evaluation results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualization(output,file_number:int, gt:bool = False):\n",
    "  # ############################\n",
    "  # visualization aggregation\n",
    "  # ############################\n",
    "  '''\n",
    "  file_number : must be cpu integer\n",
    "  '''\n",
    "  colors = ['red','blue','green','yellow','purple']\n",
    "  fig, ax = plt.subplots()\n",
    "  if gt == True:\n",
    "    img_scale = output['img_scale']\n",
    "    output = output['bbox'][0]\n",
    "    output *= img_scale\n",
    "    for bi in range(len(output)):\n",
    "      box = output[bi]\n",
    "      b = np.array(box.cpu())\n",
    "      ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "          # 좌표변경 yxyx -> xywh\n",
    "          (b[1],b[0]),b[3]-b[1] ,b[2]-b[0] , edgecolor = 'red', fill=False)\n",
    "      )\n",
    "  else:\n",
    "    for bi in range(len(output)): # num of box\n",
    "      box = output[bi]\n",
    "      b = np.array(box.cpu())\n",
    "      color_index = int(b[6])\n",
    "      if color_index ==-1: # integ만 확인\n",
    "        linewidth=2\n",
    "      else:\n",
    "        linewidth=1\n",
    "      ax.add_patch(\n",
    "        patches.Rectangle(\n",
    "          # 좌표변경 xyxy -> xywh\n",
    "          (b[0],b[1]),b[2]-b[0] ,b[3]-b[1] , edgecolor = colors[color_index], fill=False,linewidth = linewidth)\n",
    "      \n",
    "      )\n",
    "      '''\n",
    "      centerx = b[0]+ (b[2]-b[0])/2 \n",
    "      centery = b[1]+ (b[3]-b[1])/2\n",
    "      ax.text(centerx,centery,b[5])\n",
    "      '''\n",
    "  file_number = str(file_number)\n",
    "  file_name = '0'*(12-len(file_number))+file_number\n",
    "  img = Image.open('data/val2017/'+file_name+'.jpg')\n",
    "  ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregation(width,height,num_gpu,output,overlap_ratio = 0.25):\n",
    "  # #########################################\n",
    "  # aggregation\n",
    "  # #########################################\n",
    "  if overlap_ratio==0.25:\n",
    "    sub_width = torch.tensor(4*width/(3*int(num_gpu/2)+1),dtype=int) # 분할된 이미지 크기, [batch,1]\n",
    "    sub_height = torch.tensor(4*height/(3*int(num_gpu/2)+1),dtype=int)\n",
    "  elif overlap_ratio == 0.3:\n",
    "    sub_width = torch.tensor(width/(2-overlap_ratio),dtype=int)\n",
    "    sub_height = torch.tensor(height/(2-overlap_ratio),dtype=int)\n",
    "  elif overlap_ratio == 0.5:\n",
    "    sub_width = torch.tensor(width/(2-overlap_ratio),dtype=int)\n",
    "    sub_height = torch.tensor(height/(2-overlap_ratio),dtype=int)\n",
    "  else: # overlap = 0\n",
    "    sub_width = torch.tensor(width/int(num_gpu/2),dtype=int)\n",
    "    sub_height = torch.tensor(height/int(num_gpu/2),dtype=int)\n",
    "\n",
    "  #output : [batch*num_gpu,40,6]\n",
    "  batch_size = int(output.size()[0]/num_gpu)\n",
    "  num_det_img = output.size()[1]\n",
    "\n",
    "  ext_width = width-sub_width\n",
    "  ext_height = height-sub_height\n",
    "  # [batch,1]\n",
    "  ext_width = ext_width.unsqueeze(dim=1) \n",
    "  ext_height = ext_height.unsqueeze(dim=1) \n",
    "  # [batch, 1,1]\n",
    "  ext_width = torch.cat([ext_width for _ in range(num_det_img)],dim=1)\n",
    "  ext_height = torch.cat([ext_height for _ in range(num_det_img)],dim=1)\n",
    "  # [batch, 40,1]\n",
    "  \n",
    "  outputs = list(torch.split(output,batch_size,dim=0))\n",
    "  # outputs [num_gpu,batch_size,40,6]\n",
    "\n",
    "  full_output=outputs[0] # 3개씩만 처리해보기\n",
    "  \n",
    "  for n in range(1,num_gpu):\n",
    "    if n==1: #4등분 기준으로 처리, 각 파티션마다 x,y값 조정\n",
    "      #outputs[n] = [batch_size,40,6]\n",
    "      #outputs[n][:,:,:1] = [batch_size,40,1]\n",
    "      #ext_width = [batch_size,40,1]\n",
    "      outputs[n] = torch.cat([outputs[n][:,:,:1]+ext_width,\n",
    "      outputs[n][:,:,1:2],\n",
    "      outputs[n][:,:,2:3]+ext_width,\n",
    "      outputs[n][:,:,3:]],dim=2)\n",
    "    elif n==2:\n",
    "      outputs[n] = torch.cat([outputs[n][:,:,:1],\n",
    "      outputs[n][:,:,1:2]+ext_height,\n",
    "      outputs[n][:,:,2:3],\n",
    "      outputs[n][:,:,3:4]+ext_height,\n",
    "      outputs[n][:,:,4:]],dim=2)\n",
    "    else:\n",
    "      outputs[n] = torch.cat([outputs[n][:,:,:1]+ext_width,\n",
    "      outputs[n][:,:,1:2]+ext_height,\n",
    "      outputs[n][:,:,2:3]+ext_width,\n",
    "      outputs[n][:,:,3:4]+ext_height,\n",
    "      outputs[n][:,:,4:]],dim=2)\n",
    "    full_output= torch.cat((full_output,outputs[n]),dim=1)\n",
    "  # full_output [batch_size,160,6]\n",
    "\n",
    "  # partition index 추가\n",
    "  gpu_index = torch.zeros([batch_size,num_det_img*num_gpu,1],device='cuda:0')\n",
    "  for i in range(num_gpu):\n",
    "    gpu_index[:,i*num_det_img : (i+1)*num_det_img,:] = i\n",
    "  full_output = torch.cat([full_output,gpu_index],dim=2)\n",
    "  return full_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(full_output,max_det_per_image=100):\n",
    "\n",
    "  boxes = full_output[0,:,:4]\n",
    "  scores = full_output[0,:,4:5].squeeze(dim=1)\n",
    "  classes = full_output[0,:,5:6].squeeze(dim=1)\n",
    "  gpu_index = full_output[0,:,6:].squeeze(dim=1)\n",
    "\n",
    "  top_detection_idx = batched_nms(boxes, scores, classes, iou_threshold=0.5)\n",
    "  top_detection_idx = top_detection_idx[:max_det_per_image]\n",
    "  boxes = boxes[top_detection_idx]\n",
    "  scores = scores[top_detection_idx,None]\n",
    "  classes = classes[top_detection_idx,None]\n",
    "  gpu_index = gpu_index[top_detection_idx,None]\n",
    "  detections = torch.cat([boxes,scores,classes.float(),gpu_index],dim=1)\n",
    "  num_det = len(top_detection_idx)\n",
    "  if num_det < max_det_per_image:\n",
    "        detections = torch.cat([\n",
    "            detections,\n",
    "            torch.zeros((max_det_per_image - num_det, 7), device=detections.device, dtype=detections.dtype)\n",
    "        ], dim=0)\n",
    "  return detections\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_nms(full_output,width,height,overlap_ratio = 0.25):\n",
    "  \n",
    "\n",
    "  def _upcast(t) :\n",
    "    # Protects from numerical overflows in multiplications by upcasting to the equivalent higher type\n",
    "    if t.is_floating_point():\n",
    "        return t if t.dtype in (torch.float32, torch.float64) else t.float()\n",
    "    else:\n",
    "        return t if t.dtype in (torch.int32, torch.int64) else t.int()\n",
    "  \n",
    "  def box_area(boxes):\n",
    "    \"\"\"\n",
    "    Computes the area of a set of bounding boxes, which are specified by their\n",
    "    (x1, y1, x2, y2) coordinates.\n",
    "\n",
    "    Args:\n",
    "        boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
    "            are expected to be in (x1, y1, x2, y2) format with\n",
    "            ``0 <= x1 < x2`` and ``0 <= y1 < y2``.\n",
    "\n",
    "    Returns:\n",
    "        Tensor[N]: the area for each box\n",
    "    \"\"\"\n",
    "    boxes = _upcast(boxes)\n",
    "    return (boxes[2] - boxes[ 0]) * (boxes[3] - boxes[1])\n",
    "  \n",
    "  def box_iou(boxes1, boxes2):\n",
    "    \"\"\"\n",
    "    boxes1,boxes2  : tensor ( [N,4] )\n",
    "    \"\"\"\n",
    "    #area1 = box_area(boxes1)\n",
    "    #area2 = box_area(boxes2)\n",
    "\n",
    "    lt = torch.max(boxes1[None, :2], boxes2[:2])  # [N,M,2]\n",
    "    rb = torch.min(boxes1[None, 2:], boxes2[2:])  # [N,M,2]\n",
    "    \n",
    "    wh = _upcast(rb - lt).clamp(min=0)  # [N,M,2]\n",
    "    inter = wh[ :, 0] * wh[ :, 1]  # [N,M]\n",
    "\n",
    "    #union = area1[:, None] + area2 - inter\n",
    "\n",
    "    #iou = inter/union\n",
    "    #return iou\n",
    "    return inter\n",
    "  def make_new_box(boxes,chosen_boxes):\n",
    "\n",
    "    b0 = min(boxes[chosen_boxes][:,0])\n",
    "    b1 = min(boxes[chosen_boxes][:,1])\n",
    "    b2 = max(boxes[chosen_boxes][:,2])\n",
    "    b3 = max(boxes[chosen_boxes][:,3])\n",
    "\n",
    "    return torch.tensor([b0, b1, b2, b3],device='cuda:0')\n",
    "  \n",
    "  def overlap(boxes,scores,classes, gpu_index, width, height, overlap_ratio):\n",
    "\n",
    "    # [M, N], M is selected box\n",
    "    \n",
    "    sorted_index = torch.argsort(classes,dim=0,descending=True)\n",
    "    boxes = boxes[sorted_index].squeeze(dim=1)\n",
    "    scores = scores[sorted_index].squeeze(dim=1)\n",
    "    classes = classes[sorted_index].squeeze(dim=1)\n",
    "    gpu_index = gpu_index[sorted_index].squeeze(dim=1)\n",
    "\n",
    "    slack = 10\n",
    "    st_flag = 0\n",
    "    while len(boxes):\n",
    "      chosen_boxes = [-1 for _ in range(4)] # 선택박스 그룹, 4는 파티션 수\n",
    "      chosen_boxes[int(gpu_index[0][0])] = 0 # 첫번째 box 입력, index로 관리\n",
    "\n",
    "      for box_index in range(len(boxes)):\n",
    "        # 같은 파티션이 아니고 유사한 위치에 넓이 또는 높이가 유사한 경우 (범위 픽셀), overlap 조건 필요\n",
    "        if gpu_index[box_index] == gpu_index[0]: #같은 파티션 패스\n",
    "          continue\n",
    "        if chosen_boxes[int(gpu_index[box_index][0])] != -1: # 높은스코어부터 결정하기 때문에 이미 존재하면 패스\n",
    "          continue\n",
    "        terminate = False\n",
    "        for chosen_index in chosen_boxes: #위 조건에 안걸리면 box 비교\n",
    "          if chosen_index==-1: # 비어있는경우\n",
    "            continue\n",
    "\n",
    "          if box_iou(boxes[box_index],boxes[chosen_index]) == 0: \n",
    "            # overlap 존재안하는경우\n",
    "            continue\n",
    "          \n",
    "          if ((abs(boxes[box_index][0]-boxes[chosen_index][0]) <slack and abs(boxes[box_index][2]-boxes[chosen_index][2]) <slack)\\\n",
    "            or (abs(boxes[box_index][1]-boxes[chosen_index][1]) <slack and abs(boxes[box_index][3]-boxes[chosen_index][3]) <slack))\\\n",
    "              ==False:\n",
    "            # 유사한 위치에 넓이 또는 높이가 유사하지 않은 경우\n",
    "            continue\n",
    "          \n",
    "          \n",
    "          # 엄격한 기준, overlap 영역 최대일때만 취합\n",
    "          lt = torch.max(boxes[box_index][None, :2], boxes[chosen_index][:2])  # [N,M,2]\n",
    "          rb = torch.min(boxes[box_index][None, 2:], boxes[chosen_index][2:])\n",
    "          overlap_length = abs(lt-rb)\n",
    "          if int( abs(gpu_index[box_index][0]-gpu_index[chosen_index][0]) ) ==1:\n",
    "            if int(abs(overlap_length[0][0] - (2*width/(2-overlap_ratio)-width) ) ) > 10:\n",
    "              continue\n",
    "          else:\n",
    "            if int(abs(overlap_length[0][1] - (2*height/(2-overlap_ratio)-height) ) ) > 10:\n",
    "              continue\n",
    "          \n",
    "\n",
    "\n",
    "          if scores[box_index][0]-scores[chosen_index][0]>0.2:\n",
    "            terminate = True\n",
    "            break \n",
    "          # 결합할 box의 class score가 0.3 이상 차이날 경우, 이 경우는 아예 제외\n",
    "          # class score로 정렬되어있기 때문에 밖 루프까지 한번에 종료\n",
    "\n",
    "          chosen_boxes[int(gpu_index[box_index][0])] = box_index # 선택\n",
    "          break\n",
    "        if terminate == True:\n",
    "          break\n",
    "\n",
    "      # chosen_boxe에 대해서 클래스별로 가장높은 box 한가지씩만 남기기\n",
    "      # -1 인것들 제거\n",
    "      chosen_boxes_f = []\n",
    "      for i in chosen_boxes:\n",
    "        if i!=-1:\n",
    "          chosen_boxes_f.append(i)\n",
    "\n",
    "      indexes = np.array([i for i in range(len(boxes))])\n",
    "      indexes = np.delete(indexes,chosen_boxes_f)\n",
    "\n",
    "      avg_scores = max(scores[chosen_boxes_f])#.mean() # score는 평균내기\n",
    "      avg_scores = torch.tensor([[avg_scores]],device='cuda:0')\n",
    "      if len(chosen_boxes_f)>1:\n",
    "        integrated_num_gpu = torch.tensor([[-1]],device='cuda:0')\n",
    "      else:\n",
    "        integrated_num_gpu = torch.tensor(gpu_index[chosen_boxes_f[0]], device='cuda:0').unsqueeze(dim=0)\n",
    "\n",
    "      integrated_classes = torch.tensor(classes[:1,:1],device='cuda:0')\n",
    "\n",
    "      if st_flag ==0:\n",
    "        new_boxes = make_new_box(boxes,chosen_boxes_f)\n",
    "        new_boxes = new_boxes.unsqueeze(dim=0)\n",
    "        new_boxes = torch.cat([new_boxes, avg_scores, integrated_classes, integrated_num_gpu],dim=1)\n",
    "      else: \n",
    "        new_box = make_new_box(boxes,chosen_boxes_f)\n",
    "        new_box = new_box.unsqueeze(dim=0)\n",
    "        new_box = torch.cat([new_box, avg_scores, integrated_classes, integrated_num_gpu],dim=1)\n",
    "        new_boxes = torch.cat([new_boxes,new_box],dim=0)\n",
    "        # 기존 box 제거 안하고 남겨두기\n",
    "        #temp = torch.cat([boxes[chosen_boxes_f], scores[chosen_boxes_f], classes[chosen_boxes_f], gpu_index[chosen_boxes_f]],dim=1)\n",
    "        #new_boxes = torch.cat([new_boxes,temp],dim=0)\n",
    "      \n",
    "      boxes = boxes[indexes]\n",
    "      scores = scores[indexes]\n",
    "      classes = classes[indexes]\n",
    "      gpu_index = gpu_index[indexes]\n",
    "\n",
    "      st_flag =1\n",
    "    return new_boxes\n",
    "\n",
    "  #full_output = [10,160,7]\n",
    "  \n",
    "  boxes = full_output[:,:,:4]\n",
    "  scores = full_output[:,:,4:5]\n",
    "  classes = full_output[:,:,5:6]\n",
    "  gpu_index = full_output[:,:,6:]\n",
    "  # [batch_size, num_det_img*num_tpu, n]\n",
    "\n",
    "  batch_size = boxes.size()[0]\n",
    "\n",
    "  temp_box = []\n",
    "  for bt in range(batch_size):\n",
    "    temp = []\n",
    "    for index, class_id in enumerate(torch.unique(classes[bt])):\n",
    "      cur_indices = torch.where(classes[bt]==class_id)[0]\n",
    "      temp.append( overlap(boxes[bt][cur_indices], scores[bt][cur_indices],classes[bt][cur_indices], gpu_index[bt][cur_indices],width,height,overlap_ratio) )\n",
    "    #print( torch.cat(temp,dim=0).unsqueeze(dim=0).size() )\n",
    "    temp_box.append( nms( torch.cat(temp,dim=0).unsqueeze(dim=0) ).unsqueeze(dim=0) )\n",
    "  \n",
    "  \n",
    "  new_boxes = torch.cat(temp_box,dim=0)\n",
    "  return new_boxes\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(args):\n",
    "    setup_default_logging()\n",
    "\n",
    "    if args.amp:\n",
    "        if has_native_amp:\n",
    "            args.native_amp = True\n",
    "        elif has_apex:\n",
    "            args.apex_amp = True\n",
    "    assert not args.apex_amp or not args.native_amp, \"Only one AMP mode should be set.\"\n",
    "    args.pretrained = args.pretrained or not args.checkpoint  # might as well try to validate something\n",
    "    args.prefetcher = not args.no_prefetcher\n",
    "\n",
    "    # create model\n",
    "    with set_layer_config(scriptable=args.torchscript):\n",
    "        extra_args = {}\n",
    "        if args.img_size is not None:\n",
    "            extra_args = dict(image_size=(args.img_size, args.img_size))\n",
    "        # img size는 512*512보다 작을경우 패딩\n",
    "        img_resize={'d2':768,'d3':896,'d4':1024,'d7':1536}\n",
    "        img_resize = img_resize[args.model[-2:]]\n",
    "        max_detection_points = int((5000/args.num_gpu)*(img_resize/(1536/2))**2)\n",
    "        max_det_per_image = int((100/args.num_gpu)*(img_resize/(1536/2))**2)\n",
    "        print('max_detection_points',max_detection_points)\n",
    "        print('max_det_per_image',max_det_per_image)\n",
    "        extra_args = dict(max_detection_points = max_detection_points, max_det_per_image =max_det_per_image) #분할 이미지 크기, 0.25 overlap일 경우\n",
    "        #extra_args = dict(max_detection_points = )\n",
    "\n",
    "        bench = create_model(\n",
    "            args.model,\n",
    "            bench_task='predict',\n",
    "            num_classes=args.num_classes,\n",
    "            pretrained=args.pretrained,\n",
    "            redundant_bias=args.redundant_bias,\n",
    "            soft_nms=args.soft_nms,\n",
    "            checkpoint_path=args.checkpoint,\n",
    "            checkpoint_ema=args.use_ema,\n",
    "            **extra_args,\n",
    "        )\n",
    "    model_config = bench.config\n",
    "\n",
    "    param_count = sum([m.numel() for m in bench.parameters()])\n",
    "    print('Model %s created, param count: %d' % (args.model, param_count))\n",
    "\n",
    "    bench = bench.cuda()\n",
    "\n",
    "    amp_autocast = suppress\n",
    "    if args.apex_amp:\n",
    "        bench = amp.initialize(bench, opt_level='O1')\n",
    "        print('Using NVIDIA APEX AMP. Validating in mixed precision.')\n",
    "    elif args.native_amp:\n",
    "        amp_autocast = torch.cuda.amp.autocast\n",
    "        print('Using native Torch AMP. Validating in mixed precision.')\n",
    "    else:\n",
    "        print('AMP not enabled. Validating in float32.')\n",
    "\n",
    "    #''' custom module test\n",
    "    if args.num_gpu > 1:\n",
    "        bench = torch.nn.DataParallel(bench, device_ids=list(range(args.num_gpu)))\n",
    "    #'''\n",
    "\n",
    "    #print('@@debug@@: ',args.dataset,args.root,args.split) # 디버그 : split_coco, data/split, val\n",
    "\n",
    "    datasets=[] # 분산 데이터셋 생성\n",
    "    for i in range(args.num_gpu):\n",
    "      datasets.append( create_dataset(args.dataset, args.root+'/split_'+str(i), args.split) ) # 데이터셋 생성\n",
    "    \n",
    "    input_config = resolve_input_config(args, model_config)\n",
    "\n",
    "    loaders =  [] # 분산 로더 생성\n",
    "    for i in range(args.num_gpu):\n",
    "      loaders.append( create_loader(\n",
    "          datasets[i],\n",
    "          input_size=input_config['input_size'],\n",
    "          batch_size=args.batch_size,\n",
    "          use_prefetcher=args.prefetcher,\n",
    "          interpolation=input_config['interpolation'],\n",
    "          fill_color=input_config['fill_color'],\n",
    "          mean=input_config['mean'],\n",
    "          std=input_config['std'],\n",
    "          num_workers=args.workers,\n",
    "          pin_mem=args.pin_mem)\n",
    "          )\n",
    "    \n",
    "    \n",
    "    # ground truth 용 데이터셋, 로더\n",
    "    gt_dataset = create_dataset('coco','data','val')\n",
    "    gt_loader = create_loader(gt_dataset, \n",
    "    input_size=input_config['input_size'],\n",
    "          batch_size=args.batch_size,\n",
    "          use_prefetcher=args.prefetcher,\n",
    "          interpolation=input_config['interpolation'],\n",
    "          fill_color=input_config['fill_color'],\n",
    "          mean=input_config['mean'],\n",
    "          std=input_config['std'],\n",
    "          num_workers=args.workers,\n",
    "          pin_mem=args.pin_mem)\n",
    "    gt_loader = iter(gt_loader)\n",
    "    \n",
    "\n",
    "    # loader 출력해서 상태살피기\n",
    "    # 가장 쉬운 방법은 loader를 4개만들어서 출력**\n",
    "\n",
    "    evaluator = create_evaluator(args.dataset, datasets[0], pred_yxyx=False) # evaluator.py\n",
    "    #첫번째만 사용해서 체크\n",
    "    \n",
    "    bench.eval()\n",
    "    batch_time = AverageMeter()\n",
    "    end = time.time()\n",
    "    last_idx = len(loaders[0]) - 1\n",
    "\n",
    "    for i in range(len(loaders)):\n",
    "      loaders[i] = iter(loaders[i])\n",
    "    \n",
    "    load_time = []\n",
    "    detect_time = []\n",
    "    aggregate_time = []\n",
    "    over_time=[]\n",
    "\n",
    "    # overlap ratio\n",
    "    overlap_ratio = int(args.root[-2:])/100\n",
    "    print('overlap_ratio',overlap_ratio)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(last_idx+1):\n",
    "            \n",
    "            time1 = time.time()\n",
    "            # input 생성\n",
    "            input, target = next(loaders[0])\n",
    "            #gt_input, gt_target = next(gt_loader)\n",
    "            # input [batch,3,512,512]\n",
    "            # target key 안에 batch 만큼의 value\n",
    "            # image size\n",
    "            width = target['img_size'][:args.batch_size,:1]\n",
    "            height = target['img_size'][:args.batch_size,1:]\n",
    "            # width, height [batch,1]\n",
    "            \n",
    "            # target도 parallel 가능하게 변경\n",
    "            for key in target.keys():\n",
    "              temp = target[key]\n",
    "              for _ in range(1,args.num_gpu):\n",
    "                temp = torch.cat((temp,target[key]),dim=0)\n",
    "              target[key] = temp\n",
    "            \n",
    "            #print('input:',input.size())\n",
    "            for n in range(1,args.num_gpu):\n",
    "              _input,_target = next(loaders[n])\n",
    "              input = torch.cat( (input, _input), dim=0)\n",
    "            \n",
    "            time2 = time.time()\n",
    "\n",
    "\n",
    "            with amp_autocast():\n",
    "                output = bench(input , img_info=target) # bench.py DetBenchPredict에서 아웃풋 합치는 것 확인\n",
    "                #img_info 보낼 경우 scale된 결과 return, eval에서는 이걸 사용해야 할수있음\n",
    "                #output = bench (input)# visualization에서는 이것\n",
    "            #print('output:',output.size()) # 4, 40, 6\n",
    "            \n",
    "            time3 = time.time()\n",
    "            # aggregation\n",
    "            outputs = aggregation(width,height,args.num_gpu,output,overlap_ratio=overlap_ratio)\n",
    "            \n",
    "            ### visualization\n",
    "            '''\n",
    "            visualization(outputs[0],int(target['file_name'][0].cpu()))\n",
    "            '''\n",
    "            time4 = time.time()\n",
    "\n",
    "            # overlap box integration\n",
    "            #outputs = overlap_nms(outputs,width,height,overlap_ratio=overlap_ratio)\n",
    "            \n",
    "            \n",
    "            # test for overlap but non integration\n",
    "            \n",
    "            list_output =[]\n",
    "            for bt in range(args.batch_size):\n",
    "              list_output.append( nms(outputs[bt:bt+1]).unsqueeze(dim=0) )\n",
    "            outputs = torch.cat(list_output,dim=0)\n",
    "            \n",
    "\n",
    "            time5 = time.time()\n",
    "\n",
    "            ### visualization\n",
    "            '''\n",
    "            for bt in range(args.batch_size):\n",
    "              visualization(outputs[bt],int(target['file_name'][bt].cpu()))\n",
    "            '''\n",
    "\n",
    "            #eval 에서는 gt_target으로 비교\n",
    "            gt_input, gt_target = next(gt_loader)\n",
    "            evaluator.add_predictions(outputs, gt_target)\n",
    "            '''\n",
    "            ### ground truth visualization\n",
    "            visualization(gt_target,int(target['file_name'][0].cpu()),gt = True)\n",
    "            '''\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % args.log_freq == 0 or i == last_idx:\n",
    "                print(\n",
    "                    'Test: [{0:>4d}/{1}]  '\n",
    "                    'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    .format(\n",
    "                        i, last_idx+1, batch_time=batch_time,\n",
    "                        rate_avg=input.size(0) / batch_time.avg)\n",
    "                )\n",
    "            \n",
    "            load_time.append(time2-time1)\n",
    "            detect_time.append(time3-time2)\n",
    "            aggregate_time.append(time4-time3)\n",
    "            over_time.append(time5-time4)\n",
    "            if i==0:\n",
    "              break\n",
    "    print('@@@@@@@@@@@@@@@')\n",
    "    print('로드', np.mean(load_time))\n",
    "    print('탐지', np.mean(detect_time[1:]))\n",
    "    print('취합', np.mean(aggregate_time))\n",
    "    print('박스처리', np.mean(over_time))\n",
    "    print('@@@@@@@@@@@@@@@')\n",
    "    \n",
    "            \n",
    "    mean_ap = 0.\n",
    "    if datasets[0].parser.has_labels:\n",
    "        mean_ap = evaluator.evaluate(output_result_file=args.results)\n",
    "    else:\n",
    "        evaluator.save(args.results)\n",
    "\n",
    "    return mean_ap\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_detection_points 1250\n",
      "max_det_per_image 25\n",
      "Model tf_efficientdet_d2 created, param count: 8097039\n",
      "AMP not enabled. Validating in float32.\n",
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.57s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.71s)\n",
      "creating index...\n",
      "index created!\n",
      "overlap_ratio 0.0\n",
      "Test: [   0/5000]  Time: 1.906s (1.906s,    2.10/s)  \n",
      "@@@@@@@@@@@@@@@\n",
      "로드 1.1066539287567139\n",
      "탐지 nan\n",
      "취합 0.00081634521484375\n",
      "박스처리 0.0006880760192871094\n",
      "@@@@@@@@@@@@@@@\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.04s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.223\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.379\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.264\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.299\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.344\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.136\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.338\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22292079207920792"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args_str = 'data/split_00 --dataset split_coco --model tf_efficientdet_d2 --num-gpu 4 --batch-size 1'\n",
    "#args_str = 'data --model tf_efficientdet_d0 --num-gpu 4'\n",
    "args,_ = parser.parse_known_args(args=args_str.split())\n",
    "validate(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1609c5d6daf415add5fd1f90da23d5406f9079b60a4e5640100862a3157a35d6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('t': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "1609c5d6daf415add5fd1f90da23d5406f9079b60a4e5640100862a3157a35d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
